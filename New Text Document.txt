The \textit{Student's t-test} is a statistic for measuring the significance of a difference of means. The general use is as follows. Take one set of measurements, then some event happens, then take another set of measurements. Did the event, like a change in a control parameter, make a difference? In this case the measurements are the final-epoch loss scores from the ensemble of models, and the event is a change in the model train data, minute-by-minute to top-of-hour. There are several variations of the \textit{Student's t-test} including the \textit{independent t-test} for equal and unequal variances, and the \textit{dependent t-test} for paired samples. The \textit{independent t-test} for unequal variances is used in this study because the samples are independent (not paired) and the variances are not assumed to be equal based on the loss curves in \fig{fig:ensemble_loss_curves_with_solar_angle}. This specific test is known as \textit{Welch's t-test} and defines the statistic \textit{t} by
\begin{equation}
t = \frac{\bar{X_{1}} - \bar{X_{2}}}{\sqrt{\frac{s_{1}^{2}}{N_{1}} + \frac{s_{2}^{2}}{N_{2}}}},
\end{equation}
where $\bar{X_{j}}$, $s_{j}$ and $N_{j}$ are the $j^{th}$ sample mean, sample standard deviation and sample size, respectively, and $j$ is 1 and 2. The degrees of freedom, $\nu$, associated with this variance estimate is approximated by the \textit{Welch-Sattethwaite equation}:
\begin{equation}
\nu \approx \frac{\left(\frac{s_{1}^{2}}{N_{1}} + \frac{s_{2}^{2}}{N_{2}}\right)^{2}}{\frac{s_{1}^{4}}{N_{1}^{2}\nu_{1}} + \frac{s_{2}^{4}}{N_{2}^{2}\nu_{2}}},
\end{equation}
where $\nu_{1} = N_{1} - 1$ is the degrees of freedom associated with the first variance estimate, and $\nu_{2} = N_{2} - 1$ is the degrees of freedom associated with the second variance estimate. One $t$ and $\nu$ have been computed, a two-tailed test is applied to these statistics to test the null hypothesis that the two means are equal. From this test, a p-value is returned. If the p-value is large, for example larger than a user-defined threshold of 0.05 or 0.1 than we cannot reject the null hypothesis that the averages are identical. If the p-value is smaller than the user-defined threshold then we reject the null hypothesis that the averages are equal. This means the averages are different to a high level of significance and are unlikely to be different by chance.

The \textit{independent t-test} is applied using the robust implementation from \textit{scipy}. The arguments passed into the function are the 20 final-epoch loss scores from the minute-by-minute models and 20 final-epoch loss scores from the top-of-hour models, and a flag indicating the variances are not assumed to be equal. The \textit{scipy} function handles all the calculations and returns the p-value. The first test performed is on the final-epoch \textbf{test} loss scores from the ensemble of models and the resulting p-value is 0.224 which means we cannot reject the null hypothesis that the averages are equal, i.e., with statistical significance we cannot say the minute-by-minute model is better than the top-of-hour model when applied to the test dataset. The \textit{student's t-test} is also applied to the ensemble of final-epoch loss scores from the \textbf{train} dataset. The resulting p-value is 0.001 which means at the 1\% level we can reject the null hypothesis that the averages of the train loss scores are equal, i.e., with statistical significance we can say the minute-by-minute model is better than the top-of-hour model when applied to the train dataset. These two results indicate that modeling with the minute-by-minute data yields a significant performance improvement when the trained model is applied to the train dataset, but this improvement is not translating to this particular test dataset. However, using the minute-by-minute dataset is not hurting modeling performance as indicated by the average test loss score being 0.0055 better than the top-of-hour model. The performance difference is just not statistically significant.