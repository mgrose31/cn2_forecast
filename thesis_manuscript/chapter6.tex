\chapter{Summary and Future Work}
\label{ch6}

\section{Summary}
This thesis has meticulously stepped through the process of training a machine learning turbulence ($C_{n}^{2}$) model to forecast 4 hours of future conditions at 30-minute intervals from prior environmental measurements. The objective of this thesis, \emph{forecasting} turbulence conditions, is novel, as is the approach. The technique uses a turbulence sensor and weather station deployed for about two months, carefully formats the data into sequence/forecast pairs, then trains a Recurrent Neural Network that is shown to accurately predict the general trend of future turbulence conditions from the prior environmental measurements.

The grid search resulted in the selection of the \ac{GRU-RNN} model based on ensemble model performance with assistance from the \textit{Student's t-test}. This result is consistent with the expectation that a \ac{RNN} architecture is better than the \ac{MLP} because it processes the input data as a time series. The model requires only 12 hours of prior weather and $C_{n}^{2}$ measurements at 30-minute intervals. The weather measurements (pressure, relative humidity, and solar irradiance) are from a station nearly 9 km away from the turbulence sensor platform. The technique for filling nighttime turbulence measurements is a suitable method for retaining the ability to provide morning forecasts when nighttime measurements are not available. The model is effective for a nine day test window given a very small dataset of only 1107 sequence/forecast pairs for training. Model training time is only a few seconds so computation requirements are minimal.

\section{Future Work}
This thesis proposed a novel idea and presented basic techniques to achieve its goal. For example, the nighttime filling of $C_{n}^{2}$ measurements is a technique that appeared to be effective, but no other techniques were tested for improved performance. Another example is the initial hidden state of the \ac{RNN} architectures. Initializing the hidden state $h_{0}$ with zeros is the default behavior and the only initialization used in this thesis. The following is a comprehensive list of topics for future work encountered in this thesis.
\begin{itemize}
	\item Improve data preprocessing techniques to fill small gaps in weather or $C_{n}^{2}$ measurements. In this work a sequence/forecast pair was removed for any missing measurement. Filling small gaps with a technique like interpolation could retain many more training examples.
	\item \ac{RNN} architectures can process variable-size input sequences. Analyze how model performance is impacted by input sequences with padded, interpolated, or missing inputs.
	\item Investigate how the model is impacted by input sequences that are out beyond the dynamic range of the training examples. The dataset used in this work is nearly ideal because the test dataset is a great representation of the train dataset.
	\item Expand the grid search to iterate over many more hyperparameters like learning rate and learning rate decay, number of epochs, optimization algorithms, input variables, etc.
	\item Explore shorter (like 2 hour) and longer (like 8 hour) forecasts.
	\item Train a model on more data (like the last four months) and on less data (like the last two weeks) to analyze the data requirement for an effective model.
	\item Model $C_{n}^{2}$ at specific DELTA bins (screens), like the first or second, to isolate the forecasting to a particular part of the propagation path.
	\item Analyze the model for whether a spatial separation between weather and $C_{n}^{2}$ measurements has been learned.
	\item The \ac{RNN} was employed to process the input sequence as a time series, but the model output is simply the hidden \ac{RNN} nodes fully-connected to eight output nodes which have no temporal processing. Use a recurrent architecture as the \emph{output} to process the forecast in time.
\end{itemize}